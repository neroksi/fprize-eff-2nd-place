{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344133c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:09.950050Z",
     "iopub.status.busy": "2022-08-21T10:13:09.949545Z",
     "iopub.status.idle": "2022-08-21T10:13:09.999497Z",
     "shell.execute_reply": "2022-08-21T10:13:09.998538Z"
    },
    "papermill": {
     "duration": 0.067483,
     "end_time": "2022-08-21T10:13:10.003818",
     "exception": false,
     "start_time": "2022-08-21T10:13:09.936335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"../input/deberta-v3-tokenizer-fast\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c3c6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:10.021148Z",
     "iopub.status.busy": "2022-08-21T10:13:10.020186Z",
     "iopub.status.idle": "2022-08-21T10:13:16.423556Z",
     "shell.execute_reply": "2022-08-21T10:13:16.422622Z"
    },
    "papermill": {
     "duration": 6.412288,
     "end_time": "2022-08-21T10:13:16.426185",
     "exception": false,
     "start_time": "2022-08-21T10:13:10.013897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.convert_slow_tokenizer import SpmConverter\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2 import (\n",
    "        DebertaV2Tokenizer,\n",
    "    )\n",
    "\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "from tokenizers import Regex, normalizers, processors\n",
    "class DebertaV2Converter(SpmConverter):\n",
    "    def normalizer(self, proto):\n",
    "        list_normalizers = []\n",
    "        if self.original_tokenizer.do_lower_case:\n",
    "            list_normalizers.append(normalizers.Lowercase())\n",
    "\n",
    "        # precompiled_charsmap = proto.normalizer_spec.precompiled_charsmap\n",
    "        # if precompiled_charsmap:\n",
    "        #     list_normalizers.append(normalizers.Precompiled(precompiled_charsmap))\n",
    "        list_normalizers.append(normalizers.Replace(Regex(\" {2,}\"), \" \"))\n",
    "\n",
    "        return normalizers.Sequence(list_normalizers)\n",
    "\n",
    "    def post_processor(self):\n",
    "        return processors.TemplateProcessing(\n",
    "            single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "            pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", self.original_tokenizer.convert_tokens_to_ids(\"[CLS]\")),\n",
    "                (\"[SEP]\", self.original_tokenizer.convert_tokens_to_ids(\"[SEP]\")),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_deberta_v2_tokenizer(\n",
    "    tokenizer: DebertaV2Tokenizer\n",
    ") -> DebertaV2TokenizerFast:\n",
    "    tokenizer.vocab_file = tokenizer._tokenizer.vocab_file\n",
    "    return DebertaV2TokenizerFast(\n",
    "        tokenizer._tokenizer.vocab_file,\n",
    "        **tokenizer.init_kwargs,\n",
    "        tokenizer_object=DebertaV2Converter(tokenizer).converted()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bdcdeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.438060Z",
     "iopub.status.busy": "2022-08-21T10:13:16.437193Z",
     "iopub.status.idle": "2022-08-21T10:13:16.510120Z",
     "shell.execute_reply": "2022-08-21T10:13:16.509203Z"
    },
    "papermill": {
     "duration": 0.080901,
     "end_time": "2022-08-21T10:13:16.512233",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.431332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a612f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.523908Z",
     "iopub.status.busy": "2022-08-21T10:13:16.523113Z",
     "iopub.status.idle": "2022-08-21T10:13:16.529359Z",
     "shell.execute_reply": "2022-08-21T10:13:16.528482Z"
    },
    "papermill": {
     "duration": 0.013902,
     "end_time": "2022-08-21T10:13:16.531269",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.517367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH,\n",
    "                 TRAINED_MODEL_PATH,\n",
    "                 XGB_PATH,\n",
    "                 FOLDS,\n",
    "                 hidden_state_dimension,\n",
    "                 BATCH_SIZE,\n",
    "                 NUM_WORKERS,\n",
    "                 MAX_LEN,\n",
    "                 WINDOW_SIZE,\n",
    "                 RNN,):\n",
    "        self.DOWNLOADED_MODEL_PATH=DOWNLOADED_MODEL_PATH\n",
    "        self.TRAINED_MODEL_PATH=TRAINED_MODEL_PATH\n",
    "        self.XGB_PATH=XGB_PATH\n",
    "        self.FOLDS=FOLDS\n",
    "        self.hidden_state_dimension=hidden_state_dimension\n",
    "        self.BATCH_SIZE=BATCH_SIZE\n",
    "        self.NUM_WORKERS=NUM_WORKERS\n",
    "        self.MAX_LEN=MAX_LEN\n",
    "        self.WINDOW_SIZE=WINDOW_SIZE\n",
    "        self.RNN=RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4e1a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.542396Z",
     "iopub.status.busy": "2022-08-21T10:13:16.541854Z",
     "iopub.status.idle": "2022-08-21T10:13:16.554830Z",
     "shell.execute_reply": "2022-08-21T10:13:16.554057Z"
    },
    "papermill": {
     "duration": 0.021026,
     "end_time": "2022-08-21T10:13:16.557038",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.536012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments=[]\n",
    "\n",
    "\n",
    "experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v3-large\",\n",
    "                              BATCH_SIZE=4,\n",
    "                              TRAINED_MODEL_PATH=[f\"../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold{i}.pt\" for i in range(6)],\n",
    "                              XGB_PATH='../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb',\n",
    "                              FOLDS=np.arange(6),\n",
    "                              hidden_state_dimension=1024,\n",
    "                              NUM_WORKERS=2,\n",
    "                              MAX_LEN=1280,\n",
    "                              WINDOW_SIZE=1280,\n",
    "                              RNN='GRU'))\n",
    "\n",
    "\n",
    "\n",
    "model_paths=[f\"../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-0/fold{i}.pt\" for i in range(3)]\n",
    "model_paths+=[f\"../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-1/fold{i}.pt\" for i in range(3,6)]\n",
    "experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v2-xlarge\",\n",
    "                              BATCH_SIZE=4,\n",
    "                              TRAINED_MODEL_PATH=model_paths,\n",
    "                              XGB_PATH='../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-0',\n",
    "                              FOLDS=np.arange(6),\n",
    "                              hidden_state_dimension=1536,\n",
    "                              NUM_WORKERS=2,\n",
    "                              MAX_LEN=1280,\n",
    "                              WINDOW_SIZE=1280,\n",
    "                              RNN='GRU'))\n",
    "\n",
    "\n",
    "\n",
    "experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-xlarge\",\n",
    "                              BATCH_SIZE=4,\n",
    "                              TRAINED_MODEL_PATH=[f\"../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold{i}.pt\" for i in range(6)],\n",
    "                              XGB_PATH='../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb',\n",
    "                              FOLDS=np.arange(6),\n",
    "                              hidden_state_dimension=1024,\n",
    "                              NUM_WORKERS=2,\n",
    "                              MAX_LEN=1280,\n",
    "                              WINDOW_SIZE=1280,\n",
    "                              RNN='GRU'))\n",
    "\n",
    "\n",
    "model_paths=[f\"../input/fb-test10-deberta-v2-xlarge-pl-5th-nb0/fold{i}.pt\" for i in range(3)]\n",
    "model_paths+=[f\"../input/fb-test10-deberta-v2-xlarge-pl-5th-nb1/fold{i}.pt\" for i in range(3,6)]\n",
    "experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v2-xlarge\",\n",
    "                              BATCH_SIZE=4,\n",
    "                              TRAINED_MODEL_PATH=model_paths,\n",
    "                              XGB_PATH='../input/fb-test10-deberta-v2-xlarge-pl-5th-nb0',\n",
    "                              FOLDS=np.arange(6),\n",
    "                              hidden_state_dimension=1536,\n",
    "                              NUM_WORKERS=2,\n",
    "                              MAX_LEN=1280,\n",
    "                              WINDOW_SIZE=1280,\n",
    "                              RNN='GRU'))\n",
    "\n",
    "experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v3-large\",\n",
    "                              BATCH_SIZE=4,\n",
    "                              TRAINED_MODEL_PATH=[f\"../input/fb-test10-deberta-v3-large-pl-5th-nb/fold{i}.pt\" for i in range(6)],\n",
    "                              XGB_PATH='../input/fb-test10-deberta-v3-large-pl-5th-nb',\n",
    "                              FOLDS=np.arange(6),\n",
    "                              hidden_state_dimension=1024,\n",
    "                              NUM_WORKERS=2,\n",
    "                              MAX_LEN=1280,\n",
    "                              WINDOW_SIZE=1280,\n",
    "                              RNN='GRU'))\n",
    "\n",
    "\n",
    "# experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-large\",\n",
    "#                               BATCH_SIZE=4,\n",
    "#                               TRAINED_MODEL_PATH=[f\"../input/fb-test10-deberta-large-pl-4th-tascj0-nb/fold{i}.pt\" for i in range(6)],\n",
    "#                               XGB_PATH='../input/fbtest10debertalargepl4thtascj0xgb',\n",
    "#                               FOLDS=np.arange(6),\n",
    "#                               hidden_state_dimension=1024,\n",
    "#                               NUM_WORKERS=2,\n",
    "#                               MAX_LEN=1280,\n",
    "#                               WINDOW_SIZE=1280,\n",
    "#                               RNN='GRU'))\n",
    "\n",
    "# experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v3-large\",\n",
    "#                               BATCH_SIZE=4,\n",
    "#                               TRAINED_MODEL_PATH=[f\"../input/fb-test10-deberta-v3-large-pl-4th-nb/fold{i}.pt\" for i in range(6)],\n",
    "#                               XGB_PATH='../input/fbtest10debertav3largepl4thxgb',\n",
    "#                               FOLDS=np.arange(6),\n",
    "#                               hidden_state_dimension=1024,\n",
    "#                               NUM_WORKERS=2,\n",
    "#                               MAX_LEN=1280,\n",
    "#                               WINDOW_SIZE=1280,\n",
    "#                               RNN='GRU'))\n",
    "\n",
    "\n",
    "\n",
    "# model_paths=[f\"../input/fb-test10-deberta-v2-xlarge-pl-4th-nb0/fold{i}.pt\" for i in range(3)]\n",
    "# model_paths+=[f\"../input/fb-test10-deberta-v2-xlarge-pl-4th-nb1/fold{i}.pt\" for i in range(3,6)]\n",
    "# experiments.append(Experiment(DOWNLOADED_MODEL_PATH=\"../input/deberta-v2-xlarge\",\n",
    "#                               BATCH_SIZE=4,\n",
    "#                               TRAINED_MODEL_PATH=model_paths,\n",
    "#                               XGB_PATH='../input/fbtest10debertav2xlargepl4thxgb',\n",
    "#                               FOLDS=np.arange(6),\n",
    "#                               hidden_state_dimension=1536,\n",
    "#                               NUM_WORKERS=2,\n",
    "#                               MAX_LEN=1280,\n",
    "#                               WINDOW_SIZE=1280,\n",
    "#                               RNN='GRU'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0ce01",
   "metadata": {
    "papermill": {
     "duration": 0.004555,
     "end_time": "2022-08-21T10:13:16.566681",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.562126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3dae8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.577107Z",
     "iopub.status.busy": "2022-08-21T10:13:16.576843Z",
     "iopub.status.idle": "2022-08-21T10:13:16.600784Z",
     "shell.execute_reply": "2022-08-21T10:13:16.599952Z"
    },
    "papermill": {
     "duration": 0.031579,
     "end_time": "2022-08-21T10:13:16.602992",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.571413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n",
    "full_texts={}\n",
    "\n",
    "for essay_id in test['essay_id']:\n",
    "    with open(os.path.join(\"../input/feedback-prize-effectiveness/test\",essay_id+'.txt'),'r') as f:\n",
    "        text=f.read()\n",
    "\n",
    "    full_texts[essay_id]=text\n",
    "    \n",
    "test['label']=-1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c502ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.614459Z",
     "iopub.status.busy": "2022-08-21T10:13:16.613796Z",
     "iopub.status.idle": "2022-08-21T10:13:16.641910Z",
     "shell.execute_reply": "2022-08-21T10:13:16.641059Z"
    },
    "papermill": {
     "duration": 0.036209,
     "end_time": "2022-08-21T10:13:16.643935",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.607726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_substring_span(text, substring, min_length=10, fraction=0.999):\n",
    "    \"\"\"\n",
    "    Returns substring's span from the given text with the certain precision.\n",
    "    \"\"\"\n",
    "\n",
    "    position = text.find(substring)\n",
    "    substring_length = len(substring)\n",
    "    if position == -1:\n",
    "        half_length = int(substring_length * fraction)\n",
    "        half_substring = substring[:half_length]\n",
    "        half_substring_length = len(half_substring)\n",
    "        if half_substring_length < min_length:\n",
    "            return [-1, 0]\n",
    "        else:\n",
    "            return get_substring_span(text=text,\n",
    "                                    substring=half_substring,\n",
    "                                    min_length=min_length,\n",
    "                                    fraction=fraction)\n",
    "\n",
    "    span = [position, position+substring_length]\n",
    "    return span\n",
    "\n",
    "discourse_mapping={'Lead': 0, 'Position': 1, 'Claim': 2, 'Evidence': 3, 'Counterclaim': 4, 'Rebuttal': 5, 'Concluding Statement': 6}\n",
    "\n",
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, full_texts, train, aug=False, loss_type=\"BCELoss\", max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df['discourse_text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.discourse_type=df['discourse_type'].values\n",
    "        self.essay_ids=df['essay_id'].values\n",
    "        self.full_texts=full_texts\n",
    "        self.max_len = max_len\n",
    "        self.aug = aug\n",
    "        #self.nlp_aug=naw.SynonymAug()\n",
    "        self.train=train\n",
    "\n",
    "        self.encodings=[]\n",
    "        self.labels=[]\n",
    "        self.gather_indices=[]\n",
    "        self.discourse_ids=[]\n",
    "        self.discourse_type_ids=[]\n",
    "        self.input_id_lengths=[]\n",
    "        for key in tqdm(df['essay_id'].unique()):\n",
    "            discourses=df[df['essay_id']==key]\n",
    "            text=full_texts[key]\n",
    "            reference_text=text[:]\n",
    "\n",
    "            for discourse_text,label,id,type in zip(discourses['discourse_text'],discourses['label'],discourses['discourse_id'],discourses['discourse_type']):\n",
    "                span=get_substring_span(reference_text, discourse_text.strip())\n",
    "                text=text[:span[0]]+f\"({type} start)\"+discourse_text.strip()+f\"({type} end)\"+text[span[1]:]\n",
    "                reference_text=reference_text[:span[0]]+f\"({type} start)\"+\"*\"*(span[1]-span[0])+f\"({type} end)\"+reference_text[span[1]:]\n",
    "\n",
    "\n",
    "                #reference_text[:span[0]]+\"*\"*(span[1]-span[0])+text[span[1]:]\n",
    "\n",
    "            encoding = self.tokenizer(text,\n",
    "                                   add_special_tokens=True,\n",
    "                                   max_length=self.max_len,\n",
    "                                   padding=False,\n",
    "                                   return_offsets_mapping=True,\n",
    "                                   truncation=True)\n",
    "            gather_indices=np.ones(len(encoding['input_ids']))*-1\n",
    "            discourse_type_ids=np.zeros(len(encoding['input_ids']))\n",
    "            cnt=0\n",
    "            sample_labels=[]\n",
    "            discourse_ids=[]\n",
    "\n",
    "\n",
    "\n",
    "            for discourse_text,label,id,type in zip(discourses['discourse_text'],discourses['label'],discourses['discourse_id'],discourses['discourse_type']):\n",
    "                span=get_substring_span(text, discourse_text.strip())\n",
    "                n_tokens=0\n",
    "                # print(encoding['offset_mapping'])\n",
    "                # exit()\n",
    "                for i in range(len(gather_indices)):\n",
    "                    if encoding['offset_mapping'][i]!=(0,0) and encoding['offset_mapping'][i][0]>=span[0] and encoding['offset_mapping'][i][1]<=span[1]:\n",
    "                        gather_indices[i]=cnt\n",
    "                        discourse_type_ids[i]=discourse_mapping[type]\n",
    "                        n_tokens+=1\n",
    "                text=text[:span[0]]+\"*\"*(span[1]-span[0])+text[span[1]:]\n",
    "                # if (gather_indices==3).sum()==0:\n",
    "                #     print(gather_indices)\n",
    "                if (gather_indices==cnt).sum()>0:\n",
    "                    sample_labels.append(label)\n",
    "                    discourse_ids.append(id)\n",
    "                    cnt+=1\n",
    "                # else:\n",
    "                #     print(cnt)\n",
    "                #     print(discourse_text)\n",
    "\n",
    "            # for cnt in range(int(gather_indices.max())+1):\n",
    "            #     if (gather_indices==cnt).sum()==0:\n",
    "            #         print(gather_indices)\n",
    "            #         print(cnt)\n",
    "            #         print(len(sample_labels))\n",
    "            #         print(discourses)\n",
    "            #         gather_indices=np.ones(len(encoding['input_ids']))*-1\n",
    "            #         cnt=0\n",
    "            #         sample_labels=[]\n",
    "            #         for discourse_text,label in zip(discourses['discourse_text'],discourses['label']):\n",
    "            #             span=get_substring_span(text, discourse_text.strip())\n",
    "            #             n_tokens=0\n",
    "            #             # print(encoding['offset_mapping'])\n",
    "            #             # exit()\n",
    "            #             for i in range(len(gather_indices)):\n",
    "            #                 if encoding['offset_mapping'][i]!=(0,0) and encoding['offset_mapping'][i][0]>=span[0] and encoding['offset_mapping'][i][1]<=span[1]:\n",
    "            #                     gather_indices[i]=cnt\n",
    "            #                     n_tokens+=1\n",
    "            #             # if (gather_indices==3).sum()==0:\n",
    "            #             print(gather_indices)\n",
    "            #             if (gather_indices==cnt).sum()>0:\n",
    "            #                 sample_labels.append(label)\n",
    "            #                 cnt+=1\n",
    "            #\n",
    "            #         exit()\n",
    "\n",
    "\n",
    "            self.encodings.append(encoding)\n",
    "            self.labels.append(sample_labels)\n",
    "            self.gather_indices.append(gather_indices)\n",
    "            self.discourse_ids.append(discourse_ids)\n",
    "            self.discourse_type_ids.append(discourse_type_ids)\n",
    "            self.input_id_lengths.append(len(encoding['input_ids']))\n",
    "        sorted_indices=np.argsort(self.input_id_lengths)\n",
    "        self.encodings=[self.encodings[i] for i in sorted_indices]\n",
    "        self.labels=[self.labels[i] for i in sorted_indices]\n",
    "        self.gather_indices=[self.gather_indices[i] for i in sorted_indices]\n",
    "        self.discourse_ids=[self.discourse_ids[i] for i in sorted_indices]\n",
    "        self.discourse_type_ids=[self.discourse_type_ids[i] for i in sorted_indices]\n",
    "        self.input_id_lengths=[self.input_id_lengths[i] for i in sorted_indices]\n",
    "        \n",
    "            # print(gather_indices)\n",
    "            # print(sample_labels)\n",
    "            #\n",
    "            # print(key)\n",
    "            # print(discourses)\n",
    "        #exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # self.anchors = df['anchor'].values\n",
    "        # self.targets = df['target'].values\n",
    "        # self.contexts = df['context'].values\n",
    "        # if loss_type=='BCELoss':\n",
    "        #     self.labels = df['score'].values\n",
    "        # elif loss_type=='CrossEntropyLoss':\n",
    "        #     self.labels = df['score_map'].values\n",
    "        # elif loss_type=='OrdinalLoss':\n",
    "        #     self.labels=[]\n",
    "        #     for label in df['score_map'].values:\n",
    "        #         temp=np.zeros(4)\n",
    "        #         temp[:label]=1\n",
    "        #         self.labels.append(temp)\n",
    "            #self.labels = df['score_map'].values\n",
    "\n",
    "\n",
    "        #self.level=level\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "            # for text in\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # text=self.discourse_type[idx].lower()+'[SEP]'+self.texts[idx]\n",
    "        #\n",
    "        #\n",
    "        # encoding = self.tokenizer(text,\n",
    "        #                        self.full_texts[self.essay_ids[idx]],\n",
    "        #                        add_special_tokens=True,\n",
    "        #                        max_length=self.max_len,\n",
    "        #                        padding=False,\n",
    "        #                        return_offsets_mapping=True,\n",
    "        #                        truncation=True)\n",
    "        encoding=self.encodings[idx]\n",
    "        encoding['wids']=np.array(encoding.word_ids())\n",
    "        encoding['wids'][encoding['wids']==None]=-1\n",
    "        encoding['wids']=encoding['wids'].astype('int')\n",
    "        #encoding.sequence_ids()\n",
    "        label = self.labels[idx]\n",
    "        sequence_ids=np.array(encoding.sequence_ids())\n",
    "        sequence_ids[sequence_ids==None]=-1\n",
    "        # print(sequence_ids)\n",
    "        # exit()\n",
    "\n",
    "        data={k:torch.tensor(v, dtype=torch.long) for k,v in encoding.items()}\n",
    "        data['labels']=torch.tensor(label, dtype=torch.float)\n",
    "        data['sequence_ids']=torch.tensor(sequence_ids.astype(\"int\"))\n",
    "        data['gather_indices']=torch.tensor(self.gather_indices[idx])\n",
    "        data['discourse_ids']=self.discourse_ids[idx]\n",
    "        data['discourse_type_ids']=torch.tensor(self.discourse_type_ids[idx])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be60dc3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.655115Z",
     "iopub.status.busy": "2022-08-21T10:13:16.654828Z",
     "iopub.status.idle": "2022-08-21T10:13:16.667785Z",
     "shell.execute_reply": "2022-08-21T10:13:16.666887Z"
    },
    "papermill": {
     "duration": 0.020788,
     "end_time": "2022-08-21T10:13:16.669675",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.648887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomCollate:\n",
    "    def __init__(self,tokenizer,train=True,sliding_window=None):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.train=train\n",
    "        self.sliding_window=sliding_window\n",
    "\n",
    "    def __call__(self,data):\n",
    "        \"\"\"\n",
    "        need to collate: input_ids, attention_mask, labels\n",
    "        input_ids is padded with 1, attention_mask 0, labels -100\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        bs=len(data)\n",
    "        # print(data[0])\n",
    "        # exit()\n",
    "        lengths=[]\n",
    "        for i in range(bs):\n",
    "            lengths.append(len(data[i]['input_ids']))\n",
    "        max_len=max(lengths)\n",
    "        if self.sliding_window is not None and max_len > self.sliding_window:\n",
    "            max_len= int((np.floor(max_len/self.sliding_window-1e-6)+1)*self.sliding_window)\n",
    "\n",
    "        input_ids, attention_mask, labels, BIO_labels, discourse_labels=[],[],[],[],[]\n",
    "        sequence_ids=[]\n",
    "        gather_indices=[]\n",
    "        wids=[]\n",
    "        discourse_ids=[]\n",
    "        discourse_type_ids=[]\n",
    "        for i in range(bs):\n",
    "            input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(0,max_len-lengths[i]),value=self.tokenizer.pad_token_id))\n",
    "            attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(0,max_len-lengths[i]),value=0))\n",
    "            labels.append(data[i]['labels'])\n",
    "            sequence_ids.append(torch.nn.functional.pad(data[i]['sequence_ids'],(0,max_len-lengths[i]),value=-1))\n",
    "            gather_indices.append(torch.nn.functional.pad(data[i]['gather_indices'],(0,max_len-lengths[i]),value=-1))\n",
    "            discourse_type_ids.append(torch.nn.functional.pad(data[i]['discourse_type_ids'],(0,max_len-lengths[i]),value=0))\n",
    "            discourse_ids=discourse_ids+data[i]['discourse_ids']\n",
    "            #wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n",
    "        input_ids=torch.stack(input_ids)\n",
    "        attention_mask=torch.stack(attention_mask)\n",
    "        labels=torch.cat(labels)\n",
    "        sequence_ids=torch.stack(sequence_ids)\n",
    "        gather_indices=torch.stack(gather_indices)\n",
    "        discourse_type_ids=torch.stack(discourse_type_ids)\n",
    "        #wids=torch.stack(wids)\n",
    "\n",
    "        #offsets=[encoding[\"offset_mapping\"] for encoding in data]\n",
    "        offsets=[]\n",
    "        # print(len(offsets[0]))\n",
    "        # exit()\n",
    "\n",
    "        return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
    "        \"labels\":labels,\"sequence_ids\":sequence_ids,\"wids\":wids,\"offsets\":offsets,\n",
    "        \"sample_id\":np.arange(len(input_ids)),\"gather_indices\":gather_indices,\"discourse_ids\":discourse_ids,\n",
    "        \"discourse_type_ids\":discourse_type_ids}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262a8d2",
   "metadata": {
    "papermill": {
     "duration": 0.004587,
     "end_time": "2022-08-21T10:13:16.679058",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.674471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1371de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.689993Z",
     "iopub.status.busy": "2022-08-21T10:13:16.689686Z",
     "iopub.status.idle": "2022-08-21T10:13:16.712534Z",
     "shell.execute_reply": "2022-08-21T10:13:16.711509Z"
    },
    "papermill": {
     "duration": 0.031118,
     "end_time": "2022-08-21T10:13:16.714907",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.683789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, rnn='GRU'):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.downsample=nn.Linear(d_model,d_model//2)\n",
    "        if rnn=='GRU':\n",
    "            self.LSTM=nn.GRU(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        else:\n",
    "            self.LSTM=nn.LSTM(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.norm1= nn.LayerNorm(d_model//2)\n",
    "        self.linear1=nn.Linear(d_model//2, d_model)\n",
    "        self.linear2=nn.Linear(d_model*4, d_model)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.permute(1,0,2)\n",
    "        res=x\n",
    "        x=self.downsample(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x = self.linear1(x)\n",
    "        # x=self.dropout1(x)\n",
    "        # x=self.norm1(x)\n",
    "        # x=F.relu(self.linear1(x))\n",
    "        # x=self.linear2(x)\n",
    "        # x=self.dropout2(x)\n",
    "        x=res+x\n",
    "        x=x.permute(1,0,2)\n",
    "        return self.norm2(x)\n",
    "\n",
    "class SlidingWindowTransformerModel(nn.Module):\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH, hidden_state_dimension, nclass, rnn='GRU', window_size=512, edge_len=64, no_backbone=False):\n",
    "        super(SlidingWindowTransformerModel, self).__init__()\n",
    "        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n",
    "        self.no_backbone=no_backbone\n",
    "        if no_backbone:\n",
    "            pass\n",
    "        else:\n",
    "            self.backbone=AutoModel.from_pretrained(\n",
    "                               DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "\n",
    "        if rnn==\"GRU\" or rnn=='LSTM':\n",
    "            self.lstm=ResidualLSTM(hidden_state_dimension,rnn)\n",
    "        else:\n",
    "            self.lstm=ResNet()\n",
    "        self.classification_head=nn.Linear(hidden_state_dimension,nclass)\n",
    "        self.window_size=window_size\n",
    "        self.edge_len=edge_len\n",
    "        self.inner_len=window_size-edge_len*2\n",
    "\n",
    "        self.discourse_embedding=nn.Embedding(8,256,padding_idx=0)\n",
    "        self.downsample=nn.Linear(hidden_state_dimension+256,hidden_state_dimension)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,sequence_ids,discourse_type_ids,gather_indices,return_vectors=False,return_transformer_hidden_states=False):\n",
    "\n",
    "\n",
    "\n",
    "        # print(L)\n",
    "        # exit()\n",
    "        #x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n",
    "        #x=self.backbone.embeddings(input_ids)#+0.1*self.discourse_embedding(discourse_type_ids)\n",
    "        discourse_type_ids=self.discourse_embedding(discourse_type_ids)\n",
    "        x=input_ids\n",
    "        # x=torch.cat([x,discourse_type_ids],-1)\n",
    "        # x=self.downsample(x)\n",
    "\n",
    "        #x=torch.cat([x,])\n",
    "\n",
    "        if self.no_backbone==False:\n",
    "            B,L=input_ids.shape\n",
    "            if L<=self.window_size:\n",
    "                x=self.backbone(x,attention_mask=attention_mask,return_dict=False)[0]\n",
    "                #pass\n",
    "            else:\n",
    "                #print(\"####\")\n",
    "                #print(input_ids.shape)\n",
    "                segments=(L-self.window_size)//self.inner_len\n",
    "                if (L-self.window_size)%self.inner_len>self.edge_len:\n",
    "                    segments+=1\n",
    "                elif segments==0:\n",
    "                    segments+=1\n",
    "                x_new=self.backbone(x[:,:self.window_size],attention_mask=attention_mask[:,:self.window_size],return_dict=False)[0]\n",
    "                # print(x_new.shape)\n",
    "                # exit()\n",
    "\n",
    "                for i in range(1,segments+1):\n",
    "                    start=self.window_size-self.edge_len+(i-1)*self.inner_len\n",
    "                    end=self.window_size-self.edge_len+(i-1)*self.inner_len+self.window_size\n",
    "                    end=min(end,L)\n",
    "                    x_next=x[:,start:end]\n",
    "                    mask_next=attention_mask[:,start:end]\n",
    "                    x_next=self.backbone(x_next,attention_mask=mask_next,return_dict=False)[0]\n",
    "                    #L_next=x_next.shape[1]-self.edge_len,\n",
    "                    if i==segments:\n",
    "                        x_next=x_next[:,self.edge_len:]\n",
    "                    else:\n",
    "                        x_next=x_next[:,self.edge_len:self.edge_len+self.inner_len]\n",
    "                    #print(x_next.shape)\n",
    "                    x_new=torch.cat([x_new,x_next],1)\n",
    "                x=x_new\n",
    "                #print(start,end)\n",
    "        #print(x.shape)\n",
    "            if return_transformer_hidden_states:\n",
    "                transformer_hidden_states=x\n",
    "\n",
    "            # print(x.shape)\n",
    "            # exit()\n",
    "\n",
    "            # x=torch.cat([x,discourse_type_ids],-1)\n",
    "            # x=self.downsample(x)\n",
    "\n",
    "            #x=self.lstm(x)\n",
    "\n",
    "            #x=self.classification_head(x).squeeze(-1)\n",
    "\n",
    "            pooled_outputs=[]\n",
    "            if return_vectors:\n",
    "                vectors=[]\n",
    "            for i in range(len(x)):\n",
    "                #n_discourses=gather_indices[i].max()+1\n",
    "                # unique_gather_indices=torch.unique_consecutive(gather_indices[i])\n",
    "                # unique_gather_indices=unique_gather_indices[unique_gather_indices!=-1]\n",
    "                #\n",
    "                # #print(unique_gather_indices)\n",
    "                #\n",
    "                # for j in unique_gather_indices:\n",
    "                n_discourses=gather_indices[i].max()+1\n",
    "                tmp=[]\n",
    "                for j in range(n_discourses):\n",
    "\n",
    "\n",
    "                    vector=x[i][gather_indices[i]==j]\n",
    "                    if return_vectors:\n",
    "                        vectors.append(self.classification_head(vector))\n",
    "                    mean_vector=vector.mean(0)\n",
    "                    #max_vector,_=vector.max(0)\n",
    "                    # print(max_vector)\n",
    "                    # exit()\n",
    "                    #pooled=torch.cat([mean_vector,max_vector],-1)\n",
    "                    #pooled=mean_vector\n",
    "                    tmp.append(mean_vector)\n",
    "                    #pooled_outputs.append(pooled)\n",
    "                tmp=torch.stack(tmp)\n",
    "                tmp=self.lstm(tmp.unsqueeze(0))\n",
    "                pooled_outputs.append(tmp.squeeze(0))\n",
    "\n",
    "\n",
    "            #exit()\n",
    "            pooled_outputs=torch.cat(pooled_outputs)\n",
    "            x=pooled_outputs\n",
    "            x=self.classification_head(x).squeeze(-1)\n",
    "\n",
    "\n",
    "        else:\n",
    "            transformer_hidden_states=input_ids\n",
    "            x=self.lstm(transformer_hidden_states)\n",
    "            x=self.classification_head(x)\n",
    "\n",
    "        if return_vectors:\n",
    "            return x,vectors\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c963df5",
   "metadata": {
    "papermill": {
     "duration": 0.004551,
     "end_time": "2022-08-21T10:13:16.724169",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.719618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b904cb82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:16.735013Z",
     "iopub.status.busy": "2022-08-21T10:13:16.734718Z",
     "iopub.status.idle": "2022-08-21T10:13:17.156549Z",
     "shell.execute_reply": "2022-08-21T10:13:17.155559Z"
    },
    "papermill": {
     "duration": 0.430149,
     "end_time": "2022-08-21T10:13:17.159046",
     "exception": false,
     "start_time": "2022-08-21T10:13:16.728897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sorted_quantile(array, q):\n",
    "    array = np.array(array)\n",
    "    n = len(array)\n",
    "    index = (n - 1) * q\n",
    "    left = np.floor(index).astype(int)\n",
    "    fraction = index - left\n",
    "    right = left\n",
    "    right = right + (fraction > 0).astype(int)\n",
    "    i, j = array[left], array[right]\n",
    "    return i + (j - i) * fraction\n",
    "\n",
    "from scipy.stats import entropy\n",
    "#make features\n",
    "def get_xgb_features(train_df,prob_sequences):\n",
    "    features2calculate=[f\"instability_{i}\" for i in range(4)]+\\\n",
    "    [f\"begin_{i}\" for i in range(3)]+\\\n",
    "    [f\"end_{i}\" for i in range(3)]#+\\\n",
    "    #[\"entropy\"]\n",
    "\n",
    "    calculated_features=[]\n",
    "    for i,prob_seq in tqdm(enumerate(prob_sequences)):\n",
    "\n",
    "        tmp=[]\n",
    "        #quants = np.linspace(0,1,n_quan)\n",
    "        prob_seq=np.array(prob_seq)\n",
    "        instability = []\n",
    "        #all_quants=[] \n",
    "        tmp.append(np.diff(prob_seq[:,:],0).mean(0))\n",
    "        tmp.append([(np.diff(prob_seq[:,[1,2]].sum(1))**2).mean()])\n",
    "\n",
    "        tmp.append(prob_seq[:5,:].mean(0))\n",
    "        tmp.append(prob_seq[-5:,:].mean(0))\n",
    "\n",
    "        calculated_features.append(np.concatenate(tmp))\n",
    "\n",
    "\n",
    "    train_df[features2calculate]=calculated_features\n",
    "    train_df['len']=[len(s) for s in prob_sequences]\n",
    "\n",
    "    calculated_features=np.array(calculated_features)\n",
    "    calculated_features.shape\n",
    "\n",
    "    p_features=[]\n",
    "    n_features=[]\n",
    "    neighbor_features=['Ineffective','Adequate','Effective','discourse_type']\n",
    "    neighbor_features_values=train_df[neighbor_features].values\n",
    "    for i in tqdm(range(len(train_df))):\n",
    "        if i>1 and train_df['essay_id'].iloc[i]==train_df['essay_id'].iloc[i-1]:\n",
    "            p_features.append(neighbor_features_values[i-1])\n",
    "        else:\n",
    "            p_features.append(neighbor_features_values[i])\n",
    "\n",
    "        if i<(len(train_df)-1) and train_df['essay_id'].iloc[i]==train_df['essay_id'].iloc[i+1]:\n",
    "            n_features.append(neighbor_features_values[i+1])\n",
    "        else:\n",
    "            n_features.append(neighbor_features_values[i])\n",
    "\n",
    "    train_df[[f+\"_previous\" for f in neighbor_features]]=p_features\n",
    "    train_df[[f+\"_next\" for f in neighbor_features]]=n_features\n",
    "\n",
    "    train_df['mean_Ineffective']=train_df.groupby(\"essay_id\")[\"Ineffective\"].transform(\"mean\")\n",
    "    train_df['mean_Adequate']=train_df.groupby(\"essay_id\")[\"Adequate\"].transform(\"mean\")\n",
    "    train_df['mean_Effective']=train_df.groupby(\"essay_id\")[\"Effective\"].transform(\"mean\")\n",
    "\n",
    "    train_df['std_Ineffective']=train_df.groupby(\"essay_id\")[\"Ineffective\"].transform(\"std\")\n",
    "    train_df['std_Adequate']=train_df.groupby(\"essay_id\")[\"Adequate\"].transform(\"std\")\n",
    "    train_df['std_Effective']=train_df.groupby(\"essay_id\")[\"Effective\"].transform(\"std\")\n",
    "\n",
    "    train_df['discourse_count']=train_df.groupby(\"essay_id\")['discourse_type'].transform(\"count\")\n",
    "\n",
    "    cnts=train_df.groupby('essay_id')['discourse_type'].apply(lambda x: x.value_counts())\n",
    "\n",
    "    #new_df=[]\n",
    "    discourse_types=['Claim','Evidence','Concluding Statement','Lead','Position','Counterclaim','Rebuttal']\n",
    "    value_count_hash={}\n",
    "    for t in discourse_types:\n",
    "        value_count_hash[t]={}\n",
    "    for key in cnts.keys():\n",
    "        value_count_hash[key[1]][key[0]]=cnts[key]\n",
    "\n",
    "    discourse_cnts=[]    \n",
    "    for essay_id in train_df['essay_id'].unique():\n",
    "        row=[essay_id]\n",
    "        for d in discourse_types:\n",
    "            try:\n",
    "                row.append(value_count_hash[d][essay_id])\n",
    "            except:\n",
    "                row.append(0)\n",
    "        discourse_cnts.append(row)\n",
    "\n",
    "    discourse_cnts=pd.DataFrame(discourse_cnts,columns=['essay_id']+[f'{d}_count' for d in discourse_types])    \n",
    "    #discourse_cnts\n",
    "\n",
    "    train_df=train_df.merge(discourse_cnts,how='left',on='essay_id')\n",
    "    train_df\n",
    "\n",
    "    #train_df\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d105d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:17.170447Z",
     "iopub.status.busy": "2022-08-21T10:13:17.169871Z",
     "iopub.status.idle": "2022-08-21T10:13:17.176936Z",
     "shell.execute_reply": "2022-08-21T10:13:17.175893Z"
    },
    "papermill": {
     "duration": 0.0152,
     "end_time": "2022-08-21T10:13:17.179271",
     "exception": false,
     "start_time": "2022-08-21T10:13:17.164071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "neighbor_features=['Ineffective','Adequate','Effective','discourse_type']\n",
    "discourse_types=['Claim','Evidence','Concluding Statement','Lead','Position','Counterclaim','Rebuttal']\n",
    "\n",
    "features=[\"Ineffective\",\"Adequate\",\"Effective\",\n",
    "          \"instability_0\",\"instability_1\",\"instability_2\",\"instability_3\",\n",
    "          \"len\",\"discourse_type\"]\n",
    "features+=[f\"begin_{i}\" for i in range(3)]\n",
    "features+=[f\"end_{i}\" for i in range(3)]\n",
    "\n",
    "features=features+[f+\"_previous\" for f in neighbor_features]+[f+\"_next\" for f in neighbor_features]+\\\n",
    "['mean_Ineffective','mean_Adequate','mean_Effective']+['std_Ineffective','std_Adequate','std_Effective']+\\\n",
    "['discourse_count']+[f'{d}_count' for d in discourse_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca155ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:13:17.190249Z",
     "iopub.status.busy": "2022-08-21T10:13:17.189985Z",
     "iopub.status.idle": "2022-08-21T10:26:11.401090Z",
     "shell.execute_reply": "2022-08-21T10:26:11.400065Z"
    },
    "papermill": {
     "duration": 774.220416,
     "end_time": "2022-08-21T10:26:11.404464",
     "exception": false,
     "start_time": "2022-08-21T10:13:17.184048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.61it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/pytorch_model.bin were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "10it [00:00, 3943.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8966.02it/s]\n",
      "10it [00:00, 8103.37it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20340.95it/s]\n",
      "10it [00:00, 6775.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16307.56it/s]\n",
      "10it [00:00, 7210.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11422.40it/s]\n",
      "10it [00:00, 6082.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 5014.71it/s]\n",
      "10it [00:00, 5153.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 9012.26it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.09it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v2-xlarge/pytorch_model.bin were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.87s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "10it [00:00, 3722.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17353.35it/s]\n",
      "10it [00:00, 7487.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19544.75it/s]\n",
      "10it [00:00, 4611.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 5829.47it/s]\n",
      "10it [00:00, 4184.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15500.01it/s]\n",
      "10it [00:00, 5081.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 9238.56it/s]\n",
      "10it [00:00, 7810.62it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3338.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 67.07it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-xlarge/pytorch_model.bin were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "10it [00:00, 3615.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18816.98it/s]\n",
      "10it [00:00, 7986.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13976.35it/s]\n",
      "10it [00:00, 4888.47it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15966.14it/s]\n",
      "10it [00:00, 3119.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19301.91it/s]\n",
      "10it [00:00, 5084.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15482.85it/s]\n",
      "10it [00:00, 5983.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 5935.06it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00, 78.30it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v2-xlarge/pytorch_model.bin were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "10it [00:00, 5187.76it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15274.23it/s]\n",
      "10it [00:00, 8230.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11031.84it/s]\n",
      "10it [00:00, 6368.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11818.27it/s]\n",
      "10it [00:00, 7927.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 6450.79it/s]\n",
      "10it [00:00, 7866.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14974.31it/s]\n",
      "10it [00:00, 6884.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 10379.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 51.01it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/pytorch_model.bin were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "10it [00:00, 3014.45it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12818.78it/s]\n",
      "10it [00:00, 8190.40it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15923.71it/s]\n",
      "10it [00:00, 6053.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8687.46it/s]\n",
      "10it [00:00, 8171.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15318.86it/s]\n",
      "10it [00:00, 6091.06it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 7909.30it/s]\n",
      "10it [00:00, 4248.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8811.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from cuml import ForestInference\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "label_mapping={'Ineffective': 0, 'Adequate': 1, 'Effective': 2}\n",
    "#xgb_preds = []\n",
    "\n",
    "subs=[]\n",
    "for exp in experiments:\n",
    "    test_params = {'batch_size': exp.BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': exp.NUM_WORKERS,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "    if \"v2\" in exp.DOWNLOADED_MODEL_PATH or \"v3\" in exp.DOWNLOADED_MODEL_PATH:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(exp.DOWNLOADED_MODEL_PATH)\n",
    "        tokenizer = convert_deberta_v2_tokenizer(tokenizer)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(exp.DOWNLOADED_MODEL_PATH)\n",
    "    test_dataset = FeedbackDataset(tokenizer, test, full_texts, False, 0, ' ', exp.MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, **test_params, collate_fn=CustomCollate(tokenizer))\n",
    "    model = SlidingWindowTransformerModel(exp.DOWNLOADED_MODEL_PATH,\n",
    "                                      hidden_state_dimension=exp.hidden_state_dimension,\n",
    "                                      window_size=exp.WINDOW_SIZE,rnn=exp.RNN,\n",
    "                                      nclass=3)\n",
    "    model.to(device);\n",
    "    preds=[]\n",
    "    discourse_ids=[]\n",
    "    for index,weight_path in enumerate(exp.TRAINED_MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(weight_path))\n",
    "        model.eval()\n",
    "        tmp=[]\n",
    "        tmp_vectors=[]\n",
    "        for batch in tqdm(test_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            sequence_ids = batch['sequence_ids'].to(device, dtype = torch.long)\n",
    "            sample_id=batch['sample_id']\n",
    "            gather_indices = batch['gather_indices'].to(device, dtype = torch.long)\n",
    "            discourse_type_ids = batch['discourse_type_ids'].to(device, dtype = torch.long)\n",
    "            if index==0:\n",
    "                discourse_ids=discourse_ids+batch['discourse_ids']\n",
    "            max_sample_id=sample_id.max()\n",
    "            with torch.no_grad():\n",
    "                output,vectors = model(ids,mask,sequence_ids,discourse_type_ids,gather_indices,return_vectors=True)\n",
    "                vectors=[torch.nn.functional.softmax(v,-1).cpu().numpy() for v in vectors]\n",
    "                tmp_vectors+=vectors\n",
    "                output=torch.nn.functional.softmax(output,-1)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            tmp.append(output.cpu())\n",
    "        tmp=torch.cat(tmp)\n",
    "        preds.append(tmp)\n",
    "        \n",
    "        if index==0:\n",
    "            prob_sequences=[tmp_vectors]\n",
    "        else:\n",
    "    #         for i in range(len(prob_sequences)):\n",
    "    #             prob_sequences[i]=prob_sequences[i]+tmp_vectors[i]\n",
    "            prob_sequences.append(tmp_vectors)            \n",
    "        \n",
    "        \n",
    "    preds=torch.stack(preds).numpy()\n",
    "    \n",
    "    \n",
    "    xgb_preds=[]\n",
    "    for fold in exp.FOLDS:\n",
    "        sub=pd.DataFrame(columns=['discourse_id']+list(label_mapping.keys()))\n",
    "        sub['discourse_id']=discourse_ids\n",
    "        sub[list(label_mapping.keys())]=preds[fold]\n",
    "        sub=sub.merge(test[['discourse_id','discourse_type','essay_id']],how='left',on='discourse_id')\n",
    "\n",
    "        sub=get_xgb_features(sub,prob_sequences[fold])\n",
    "\n",
    "        for f in features:\n",
    "            if f not in ['discourse_type_previous','discourse_type_next','discourse_type']:\n",
    "                sub[f]= sub[f].astype('float')\n",
    "            else:    \n",
    "                sub[f]= sub[f].astype('category')\n",
    "\n",
    "        d_test = xgb.DMatrix(sub[features],enable_categorical=True)\n",
    "        for xgb_fold in exp.FOLDS:\n",
    "            #xgb_model_loaded = pickle.load(open(f\"{exp.XGB_PATH}/xgb_{xgb_fold}.p\", \"rb\"))\n",
    "            xgb_model_loaded = xgb.Booster()\n",
    "            xgb_model_loaded.load_model(f\"{exp.XGB_PATH}/xgb_{xgb_fold}.json\")\n",
    "            xgb_preds.append(xgb_model_loaded.predict(d_test))\n",
    "\n",
    "    xgb_preds=np.stack(xgb_preds)\n",
    "    xgb_preds.shape\n",
    "    xgb_preds=xgb_preds.mean(0)        \n",
    "    \n",
    "    submission=pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\n",
    "\n",
    "    discourse_ids=list(submission['discourse_id'])\n",
    "    \n",
    "#     submission[\"Ineffective\"]=1e-9\n",
    "#     submission[\"Adequate\"]=1e-9\n",
    "#     submission[\"Effective\"]=1e-9\n",
    "    \n",
    "    for i in range(len(sub)):\n",
    "        index=discourse_ids.index(sub['discourse_id'].iloc[i])\n",
    "        submission[\"Ineffective\"].iloc[index]=xgb_preds[i,0]\n",
    "        submission[\"Adequate\"].iloc[index]=xgb_preds[i,1]\n",
    "        submission[\"Effective\"].iloc[index]=xgb_preds[i,2]\n",
    "\n",
    "    submission.to_csv(\"submission.csv\",index=False)\n",
    "    subs.append(submission)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b81ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:26:11.454605Z",
     "iopub.status.busy": "2022-08-21T10:26:11.453683Z",
     "iopub.status.idle": "2022-08-21T10:26:11.459348Z",
     "shell.execute_reply": "2022-08-21T10:26:11.458511Z"
    },
    "papermill": {
     "duration": 0.032428,
     "end_time": "2022-08-21T10:26:11.461345",
     "exception": false,
     "start_time": "2022-08-21T10:26:11.428917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xgb_model_loaded.predict_proba(sub[features]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73513d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:26:11.510030Z",
     "iopub.status.busy": "2022-08-21T10:26:11.509242Z",
     "iopub.status.idle": "2022-08-21T10:26:11.513683Z",
     "shell.execute_reply": "2022-08-21T10:26:11.512849Z"
    },
    "papermill": {
     "duration": 0.030589,
     "end_time": "2022-08-21T10:26:11.515604",
     "exception": false,
     "start_time": "2022-08-21T10:26:11.485015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgb_model_loaded = pickle.load(open(f\"{exp.XGB_PATH}/xgb_{xgb_fold}.p\", \"rb\"))\n",
    "# xgb_model_loaded.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "780285e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:26:11.564779Z",
     "iopub.status.busy": "2022-08-21T10:26:11.563381Z",
     "iopub.status.idle": "2022-08-21T10:26:11.567830Z",
     "shell.execute_reply": "2022-08-21T10:26:11.566982Z"
    },
    "papermill": {
     "duration": 0.030627,
     "end_time": "2022-08-21T10:26:11.569769",
     "exception": false,
     "start_time": "2022-08-21T10:26:11.539142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #d_test = xgb.DMatrix(sub[features],enable_categorical=True)\n",
    "# xgb_model_loaded = xgb.Booster()\n",
    "# xgb_model_loaded.load_model(f\"{exp.XGB_PATH}/xgb_{xgb_fold}.json\")\n",
    "# #xgb_model_loaded = pickle.load(open(f\"{exp.XGB_PATH}/xgb_{xgb_fold}.p\", \"rb\"))\n",
    "# xgb_model_loaded.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a603bdc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:26:11.619463Z",
     "iopub.status.busy": "2022-08-21T10:26:11.618073Z",
     "iopub.status.idle": "2022-08-21T10:26:11.637623Z",
     "shell.execute_reply": "2022-08-21T10:26:11.636652Z"
    },
    "papermill": {
     "duration": 0.045792,
     "end_time": "2022-08-21T10:26:11.639609",
     "exception": false,
     "start_time": "2022-08-21T10:26:11.593817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.311563</td>\n",
       "      <td>0.683192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>0.031912</td>\n",
       "      <td>0.680193</td>\n",
       "      <td>0.287896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>0.090935</td>\n",
       "      <td>0.419728</td>\n",
       "      <td>0.489338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.098606</td>\n",
       "      <td>0.475707</td>\n",
       "      <td>0.425687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.179806</td>\n",
       "      <td>0.445937</td>\n",
       "      <td>0.374257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2e214524dbe3</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.558329</td>\n",
       "      <td>0.399133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84812fc2ab9f</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>0.450539</td>\n",
       "      <td>0.535079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c668ff840720</td>\n",
       "      <td>0.047245</td>\n",
       "      <td>0.529719</td>\n",
       "      <td>0.423036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739a6d00f44a</td>\n",
       "      <td>0.033323</td>\n",
       "      <td>0.518314</td>\n",
       "      <td>0.448363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcfae2c9a244</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.318126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276     0.005244  0.311563   0.683192\n",
       "1  5a88900e7dc1     0.031912  0.680193   0.287896\n",
       "2  9790d835736b     0.090935  0.419728   0.489338\n",
       "3  75ce6d68b67b     0.098606  0.475707   0.425687\n",
       "4  93578d946723     0.179806  0.445937   0.374257\n",
       "5  2e214524dbe3     0.042539  0.558329   0.399133\n",
       "6  84812fc2ab9f     0.014382  0.450539   0.535079\n",
       "7  c668ff840720     0.047245  0.529719   0.423036\n",
       "8  739a6d00f44a     0.033323  0.518314   0.448363\n",
       "9  bcfae2c9a244     0.017875  0.664000   0.318126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b95da85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:26:11.688903Z",
     "iopub.status.busy": "2022-08-21T10:26:11.688623Z",
     "iopub.status.idle": "2022-08-21T10:26:11.710090Z",
     "shell.execute_reply": "2022-08-21T10:26:11.708490Z"
    },
    "papermill": {
     "duration": 0.048535,
     "end_time": "2022-08-21T10:26:11.712563",
     "exception": false,
     "start_time": "2022-08-21T10:26:11.664028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold0.pt', '../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold1.pt', '../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold2.pt', '../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold3.pt', '../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold4.pt', '../input/test10-deberta-v3-large-pl-5th-tascj0-corrected-nb/fold5.pt']\n",
      "['../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-0/fold0.pt', '../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-0/fold1.pt', '../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-0/fold2.pt', '../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-1/fold3.pt', '../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-1/fold4.pt', '../input/test10-deberta-v2-xlarge-pl-5th-tascj0-corrected-1/fold5.pt']\n",
      "['../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold0.pt', '../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold1.pt', '../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold2.pt', '../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold3.pt', '../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold4.pt', '../input/fb-test10-deberta-xlarge-pl-5th-tascj0-nb/fold5.pt']\n",
      "['../input/fb-test10-deberta-v2-xlarge-pl-5th-nb0/fold0.pt', '../input/fb-test10-deberta-v2-xlarge-pl-5th-nb0/fold1.pt', '../input/fb-test10-deberta-v2-xlarge-pl-5th-nb0/fold2.pt', '../input/fb-test10-deberta-v2-xlarge-pl-5th-nb1/fold3.pt', '../input/fb-test10-deberta-v2-xlarge-pl-5th-nb1/fold4.pt', '../input/fb-test10-deberta-v2-xlarge-pl-5th-nb1/fold5.pt']\n",
      "['../input/fb-test10-deberta-v3-large-pl-5th-nb/fold0.pt', '../input/fb-test10-deberta-v3-large-pl-5th-nb/fold1.pt', '../input/fb-test10-deberta-v3-large-pl-5th-nb/fold2.pt', '../input/fb-test10-deberta-v3-large-pl-5th-nb/fold3.pt', '../input/fb-test10-deberta-v3-large-pl-5th-nb/fold4.pt', '../input/fb-test10-deberta-v3-large-pl-5th-nb/fold5.pt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>0.652094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>0.023835</td>\n",
       "      <td>0.739010</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>0.097193</td>\n",
       "      <td>0.440936</td>\n",
       "      <td>0.461871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.398557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.198225</td>\n",
       "      <td>0.428877</td>\n",
       "      <td>0.372898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2e214524dbe3</td>\n",
       "      <td>0.041460</td>\n",
       "      <td>0.592207</td>\n",
       "      <td>0.366333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84812fc2ab9f</td>\n",
       "      <td>0.020864</td>\n",
       "      <td>0.506750</td>\n",
       "      <td>0.472386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c668ff840720</td>\n",
       "      <td>0.061274</td>\n",
       "      <td>0.559227</td>\n",
       "      <td>0.379499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739a6d00f44a</td>\n",
       "      <td>0.052973</td>\n",
       "      <td>0.561777</td>\n",
       "      <td>0.385250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcfae2c9a244</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>0.681009</td>\n",
       "      <td>0.300947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276     0.006998  0.340908   0.652094\n",
       "1  5a88900e7dc1     0.023835  0.739010   0.237155\n",
       "2  9790d835736b     0.097193  0.440936   0.461871\n",
       "3  75ce6d68b67b     0.131291  0.470152   0.398557\n",
       "4  93578d946723     0.198225  0.428877   0.372898\n",
       "5  2e214524dbe3     0.041460  0.592207   0.366333\n",
       "6  84812fc2ab9f     0.020864  0.506750   0.472386\n",
       "7  c668ff840720     0.061274  0.559227   0.379499\n",
       "8  739a6d00f44a     0.052973  0.561777   0.385250\n",
       "9  bcfae2c9a244     0.018044  0.681009   0.300947"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights=[0.33918623, 0.43399894, 0.22681483]\n",
    "\n",
    "#weights=[0.33988562, 0.48792195, 0.17219243]\n",
    "\n",
    "for exp in experiments:\n",
    "    print(exp.TRAINED_MODEL_PATH)\n",
    "\n",
    "#weights=[0.3784224, 0.23337426, 0.12014701, 0.26805633]\n",
    "\n",
    "#weights=[0.06290397, 0.21444155, 0.07934315, 0.20454198, 0.26092175, 0.17784759]\n",
    "weights=[0.1996512, 0.22573007, 0.12467703, 0.25565791, 0.1942838]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert len(subs)==len(weights)\n",
    "\n",
    "weights=np.array(weights)\n",
    "weights=weights/weights.sum()\n",
    "submission=subs[0].copy()\n",
    "submission[\"Ineffective\"]=submission[\"Ineffective\"].values*weights[0]\n",
    "submission[\"Adequate\"]=submission[\"Adequate\"].values*weights[0]\n",
    "submission[\"Effective\"]=submission[\"Effective\"].values*weights[0]\n",
    "\n",
    "\n",
    "for sub,weight in zip(subs[1:],weights[1:]):\n",
    "    submission[\"Ineffective\"]=submission[\"Ineffective\"].values+sub[\"Ineffective\"].values*weight\n",
    "    submission[\"Adequate\"]=submission[\"Adequate\"].values+sub[\"Adequate\"].values*weight\n",
    "    submission[\"Effective\"]=submission[\"Effective\"].values+sub[\"Effective\"].values*weight\n",
    "\n",
    "submission.to_csv(\"submission.csv\",index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 792.566642,
   "end_time": "2022-08-21T10:26:15.036339",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-21T10:13:02.469697",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
